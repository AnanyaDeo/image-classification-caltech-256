{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../training_utils/')\n",
    "from data_utils import get_folders, get_class_weights\n",
    "from train_utils import train, predict\n",
    "from diagnostic_tools import top_k_accuracy, per_class_accuracy,\\\n",
    "    count_params, entropy, model_calibration, show_errors, most_confused_classes,\\\n",
    "    most_inaccurate_k_classes\n",
    "    \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16980"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folder, val_folder = get_folders()\n",
    "\n",
    "train_iterator = DataLoader(\n",
    "    train_folder, batch_size=batch_size, num_workers=4,\n",
    "    shuffle=True, pin_memory=True\n",
    ")\n",
    "\n",
    "val_iterator = DataLoader(\n",
    "    val_folder, batch_size=64, num_workers=4,\n",
    "    shuffle=False, pin_memory=True\n",
    ")\n",
    "\n",
    "# number of training samples\n",
    "train_size = len(train_folder.imgs)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_resnet18 import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w[j]: 1/number_of_samples_in_class_j\n",
    "# decode: folder name to class name (in human readable format)\n",
    "w, decode = get_class_weights(val_folder.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, criterion, optimizer = get_model(class_weights=torch.FloatTensor(w/w.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18584704"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of params in the model\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 12\n",
    "n_batches = ceil(train_size/batch_size)\n",
    "# number of cycles\n",
    "M = 1 \n",
    "# total number of optimization steps\n",
    "T = n_batches*n_epochs \n",
    "# initial learning rates\n",
    "initial1 = 1e-2\n",
    "initial2 = 1e-3\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cyclical cosine annealing\n",
    "# it changes the learning rate on every optimization step\n",
    "# 1e-6 is the minimal learning rate\n",
    "def lr_scheduler(optimizer, step):\n",
    "    \n",
    "    global initial1\n",
    "    global initial2\n",
    "    decay = np.cos(np.pi*((step - 1) % (T // M))/(T // M)) + 1.0\n",
    "    \n",
    "    # params of the last fc layer\n",
    "    for param_group in optimizer.param_groups[:2]:\n",
    "        param_group['lr'] = ((initial1 - 1e-6)*decay/2.0) + 1e-6\n",
    "    \n",
    "    # params of the last dense block\n",
    "    for param_group in optimizer.param_groups[2:]:\n",
    "        param_group['lr'] = ((initial2 - 1e-6)*decay/2.0) + 1e-6\n",
    "    \n",
    "    if (step - 1) % (T // M) == 0 and step != 1:\n",
    "        print('lr is reset')\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_losses, _ = train(\n",
    "    model, criterion, optimizer, \n",
    "    train_iterator, n_epochs, n_batches, \n",
    "    val_iterator, validation_step=531, n_validation_batches=80, \n",
    "    saving_step=None, lr_scheduler=lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss/epoch plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = [x[0] for x in all_losses]\n",
    "plt.plot(epochs, [x[1] for x in all_losses], label='train');\n",
    "plt.plot(epochs, [x[2] for x in all_losses], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, [x[3] for x in all_losses], label='train');\n",
    "plt.plot(epochs, [x[4] for x in all_losses], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, [x[5] for x in all_losses], label='train');\n",
    "plt.plot(epochs, [x[6] for x in all_losses], label='val');\n",
    "plt.legend();\n",
    "plt.xlabel('epoch');\n",
    "plt.ylabel('top5_accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get human readable class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index to class name\n",
    "decode = {val_folder.class_to_idx[k]: decode[int(k)] for k in val_folder.class_to_idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all predictions and all misclassified images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_iterator_no_shuffle = DataLoader(\n",
    "    val_folder, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predictions, val_true_targets,\\\n",
    "    erroneous_samples, erroneous_targets,\\\n",
    "    erroneous_predictions = predict(model, val_iterator_no_shuffle, return_erroneous=True)\n",
    "# erroneous_samples: images that were misclassified\n",
    "# erroneous_targets: their true labels\n",
    "# erroneous_predictions: predictions for them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of misclassified images (there are overall 5120 images in the val dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_errors = len(erroneous_targets)\n",
    "n_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logloss and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(val_true_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(val_true_targets, val_predictions.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_k_accuracy(val_true_targets, val_predictions, k=(2, 3, 4, 5, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hits = val_predictions.argmax(1) == val_true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    entropy(val_predictions[hits]), bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    entropy(val_predictions[~hits]), bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('entropy of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confidence of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_predictions[hits].max(1), bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    val_predictions[~hits].max(1), bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('confidence of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### difference between biggest and second biggest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_correct = np.sort(val_predictions[hits], 1)\n",
    "sorted_incorrect = np.sort(val_predictions[~hits], 1)\n",
    "\n",
    "plt.hist(\n",
    "    sorted_correct[:, -1] - sorted_correct[:, -2], bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    sorted_incorrect[:, -1] - sorted_incorrect[:, -2], bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probabilistic calibration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_calibration(val_true_targets, val_predictions, n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per class accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_class_acc = per_class_accuracy(val_true_targets, val_predictions)\n",
    "plt.hist(per_class_acc);\n",
    "plt.xlabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_inaccurate_k_classes(per_class_acc, 15, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class accuracy vs. number of samples in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter((1.0/w), per_class_acc);\n",
    "plt.ylabel('class accuracy');\n",
    "plt.xlabel('number of available samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most confused pairs of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confused_pairs = most_confused_classes(\n",
    "    val_true_targets, val_predictions, decode, min_n_confusions=4\n",
    ")\n",
    "confused_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### show some low entropy errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "erroneous_entropy = entropy(erroneous_predictions)\n",
    "mean_entropy = erroneous_entropy.mean()\n",
    "low_entropy = mean_entropy < erroneous_entropy\n",
    "mean_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_errors(\n",
    "    erroneous_samples[low_entropy], \n",
    "    erroneous_predictions[low_entropy], \n",
    "    erroneous_targets[low_entropy], \n",
    "    decode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### show some high entropy errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_errors(\n",
    "    erroneous_samples[~low_entropy], \n",
    "    erroneous_predictions[~low_entropy], \n",
    "    erroneous_targets[~low_entropy], \n",
    "    decode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.cpu();\n",
    "torch.save(model.state_dict(), 'resnet18.pytorch_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
